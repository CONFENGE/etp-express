# Post-Mortem: [TÃ­tulo do Incidente]

**Data do Incidente:** YYYY-MM-DD
**DuraÃ§Ã£o Total:** X horas Y minutos
**Severity:** P0 / P1 / P2
**Incident Commander:** [Nome]
**Escrito por:** [Nome]
**Revisado por:** [Nome]
**Data do Post-Mortem:** YYYY-MM-DD

---

## Executive Summary

[Resumo executivo de 2-3 parÃ¡grafos para stakeholders nÃ£o-tÃ©cnicos]

**Exemplo:**

> No dia DD/MM/YYYY Ã s HH:MM, o sistema ETP Express ficou completamente indisponÃ­vel por 45 minutos devido a uma falha no banco de dados de produÃ§Ã£o. O problema afetou 100% dos usuÃ¡rios ativos, impedindo acesso a ETPs e geraÃ§Ã£o de novas seÃ§Ãµes. A equipe identificou a root cause em 15 minutos e restaurou o serviÃ§o em 45 minutos utilizando procedimento de rollback. NÃ£o houve perda de dados. Implementamos monitoramento proativo e procedimentos de rollback automÃ¡tico para prevenir recorrÃªncia.

---

## Timeline

**Todos os horÃ¡rios em UTC-3 (BrasÃ­lia)**

| HorÃ¡rio | Evento                                                     | ResponsÃ¡vel | AÃ§Ã£o             |
| ------- | ---------------------------------------------------------- | ----------- | ---------------- |
| 15:20   | ğŸš¨ **Incident Start** - Database connection errors comeÃ§am | Sistema     | N/A              |
| 15:22   | Alert disparado no Slack #alerts-production                | Monitoring  | N/A              |
| 15:23   | First Responder acknowledges alert                         | [Nome]      | Triage inicial   |
| 15:25   | Root cause identified: PostgreSQL service crashed          | [Nome]      | DiagnÃ³stico      |
| 15:27   | Decision: Restart PostgreSQL service                       | [Nome]      | MitigaÃ§Ã£o        |
| 15:30   | PostgreSQL restart completo                                | Railway     | N/A              |
| 15:32   | Health checks passing                                      | [Nome]      | VerificaÃ§Ã£o      |
| 15:35   | Smoke tests passing (endpoints crÃ­ticos OK)                | [Nome]      | ValidaÃ§Ã£o        |
| 15:40   | Full system validation                                     | [Nome]      | Testes completos |
| 15:45   | All-clear declared                                         | [Nome]      | N/A              |
| 16:05   | ğŸ‰ **Incident End** - Users notified of resolution         | [Nome]      | ComunicaÃ§Ã£o      |

**Total duration:** 45 minutos (detection to resolution)
**MTTR (Mean Time To Resolution):** 45 minutos
**MTTD (Mean Time To Detection):** 2 minutos

---

## Impact

### Users Affected

- **Total users impacted:** [NÃºmero ou %]
  - Exemplo: "120 usuÃ¡rios ativos no momento do incidente (100%)"
  - Exemplo: "~30% da base de usuÃ¡rios (estimativa baseada em horÃ¡rio)"

### Functionality Impact

- âŒ **Completamente indisponÃ­vel:**
  - Login
  - Acesso a ETPs existentes
  - CriaÃ§Ã£o de novos ETPs
  - GeraÃ§Ã£o de seÃ§Ãµes
  - ExportaÃ§Ã£o de PDFs

- âš ï¸ **Parcialmente indisponÃ­vel:** [Se aplicÃ¡vel]
  - N/A

- âœ… **Sem impacto:** [Se aplicÃ¡vel]
  - N/A (sistema completamente fora do ar)

### Data Impact

- **Data loss:** âŒ NÃ£o / âš ï¸ Sim (detalhar abaixo)
- **Data corruption:** âŒ NÃ£o / âš ï¸ Sim (detalhar abaixo)

**Detalhes:**

- âœ… Todos os ETPs salvos antes do incidente foram preservados
- âœ… Banco de dados restaurado sem perda de informaÃ§Ãµes
- âŒ NÃ£o houve data loss

### Business Impact

- **Downtime:** 45 minutos de indisponibilidade total
- **Revenue impact:** [Se aplicÃ¡vel - geralmente N/A para MVP]
- **SLA breach:** [Se houver SLA definido]
- **Customer complaints:** [NÃºmero de tickets/emails recebidos]

---

## Root Cause

### Technical Root Cause

[ExplicaÃ§Ã£o tÃ©cnica detalhada da causa raiz]

**Exemplo:**

O serviÃ§o PostgreSQL no Railway crashou devido a um out-of-memory (OOM) error. A anÃ¡lise dos logs revelou que:

1. **Causa imediata:** PostgreSQL process consumiu 100% da memÃ³ria alocada (512MB) e foi killed pelo OOM killer do kernel
2. **Causa raiz:** Query lenta sem Ã­ndice apropriado (`SELECT * FROM sections WHERE etp_id IN (...)`) executada simultaneamente por 15 usuÃ¡rios causou memory spike
3. **Trigger:** Deploy recente (#XYZ) removeu Ã­ndice em `sections.etp_id` acidentalmente durante refatoraÃ§Ã£o de migration

### Contributing Factors

Fatores que contribuÃ­ram para o incidente ou agravaram o impacto:

1. **Falta de alerting proativo:** NÃ£o tÃ­nhamos alert de memory usage > 80%
2. **Ãndice removido sem validaÃ§Ã£o:** Migration nÃ£o passou por review de DBA
3. **Falta de load testing:** Deploy nÃ£o foi validado com carga realista antes de produÃ§Ã£o

---

## Detection

### Como foi detectado

- [x] Alerting automÃ¡tico (Monitoring)
- [ ] UsuÃ¡rio reportou problema
- [ ] Equipe descobriu durante operaÃ§Ã£o rotineira
- [ ] Outro: [Descrever]

### Tempo de detecÃ§Ã£o

- **Incident start:** 15:20 (primeira evidÃªncia nos logs)
- **Alert fired:** 15:22 (2 minutos apÃ³s inÃ­cio)
- **MTTD:** 2 minutos âœ… (objetivo: < 5 minutos)

### Qualidade da detecÃ§Ã£o

- âœ… **Boa:** Alert preciso e acionÃ¡vel
- Texto do alert: "Database connection failed - Backend health check failing"
- First Responder conseguiu iniciar triage imediatamente

---

## Response

### O que funcionou bem

âœ… **Positives:**

1. **DetecÃ§Ã£o rÃ¡pida:** Alert disparou em 2 minutos
2. **DiagnÃ³stico eficiente:** Root cause identificada em 15 minutos (follow runbook)
3. **ComunicaÃ§Ã£o clara:** UsuÃ¡rios notificados em 20 minutos do inÃ­cio
4. **Rollback procedures:** Script de rollback funcionou conforme esperado
5. **DocumentaÃ§Ã£o Ãºtil:** Runbook `INCIDENT_RESPONSE.md` guiou response corretamente

### O que nÃ£o funcionou bem

âŒ **Issues:**

1. **Falta de prevenÃ§Ã£o:** Memory spike nÃ£o foi detectado antes de OOM
2. **Migration review:** RemoÃ§Ã£o de Ã­ndice nÃ£o foi caught em code review
3. **Alerting gaps:** NÃ£o tÃ­nhamos alert de "missing index causing slow query"
4. **Testing gaps:** Load testing nÃ£o incluÃ­a cenÃ¡rio de 15+ usuÃ¡rios simultÃ¢neos

### O que tivemos sorte

ğŸ€ **Lucky breaks:**

1. Incidente ocorreu durante horÃ¡rio comercial (equipe disponÃ­vel)
2. No backup restore foi necessÃ¡rio (PostgreSQL recovery foi simples restart)
3. Nenhum usuÃ¡rio estava em processo crÃ­tico de finalizaÃ§Ã£o de ETP

---

## Resolution

### Immediate Actions Taken

1. **15:27** - Restart PostgreSQL service via Railway UI
2. **15:30** - PostgreSQL back online
3. **15:32** - Health checks validated
4. **15:35** - Smoke tests executed (critical endpoints tested)
5. **15:40** - Full regression validation
6. **15:45** - All-clear declared

### Long-term Fixes

**Implemented immediately (hotfix):**

- [ ] Re-adicionar Ã­ndice em `sections.etp_id` (Migration #XYZ-fix)
- [ ] Deploy hotfix com Ã­ndice restaurado

**Scheduled (within 7 days):**

- [ ] Implementar alert de memory usage > 80%
- [ ] Adicionar slow query logging (queries > 1s)
- [ ] Create load testing scenario com 20+ usuÃ¡rios simultÃ¢neos

**Planned (within 30 days):**

- [ ] Migration review checklist (obrigatÃ³rio incluir Ã­ndices review)
- [ ] Upgrade PostgreSQL plan (512MB â†’ 1GB RAM)
- [ ] Implement query performance monitoring (APM)

---

## Action Items

| #   | Action Item                           | Owner  | Priority | Due Date   | Status         |
| --- | ------------------------------------- | ------ | -------- | ---------- | -------------- |
| 1   | Re-adicionar Ã­ndice `sections.etp_id` | [Nome] | P0       | DD/MM/YYYY | âœ… Done        |
| 2   | Deploy hotfix com Ã­ndice              | [Nome] | P0       | DD/MM/YYYY | âœ… Done        |
| 3   | Implementar alert memory > 80%        | [Nome] | P1       | DD/MM/YYYY | ğŸ”„ In Progress |
| 4   | Adicionar slow query logging          | [Nome] | P1       | DD/MM/YYYY | ğŸ“‹ To Do       |
| 5   | Load testing com 20+ users            | [Nome] | P1       | DD/MM/YYYY | ğŸ“‹ To Do       |
| 6   | Migration review checklist            | [Nome] | P2       | DD/MM/YYYY | ğŸ“‹ To Do       |
| 7   | Upgrade PostgreSQL plan               | [Nome] | P2       | DD/MM/YYYY | ğŸ“‹ To Do       |
| 8   | Implement APM (query monitoring)      | [Nome] | P2       | DD/MM/YYYY | ğŸ“‹ To Do       |

---

## Lessons Learned

### Technical Lessons

1. **Ãndices sÃ£o crÃ­ticos:** RemoÃ§Ã£o de Ã­ndice em tabela com crescimento rÃ¡pido causa degradaÃ§Ã£o severa
2. **Memory limits matter:** PostgreSQL default settings nÃ£o sÃ£o otimizados para production workload
3. **Monitoring gaps:** Faltavam alerts proativos de resource exhaustion

### Process Lessons

1. **Migration review:** Migrations que tocam Ã­ndices precisam de review extra
2. **Load testing:** Deploy sem load testing realista = deploy blind
3. **Runbooks work:** Ter runbook documentado acelerou response em 50%+

### Communication Lessons

1. **Templates Ãºteis:** Templates de comunicaÃ§Ã£o permitiram notificaÃ§Ã£o rÃ¡pida de usuÃ¡rios
2. **Update frequency:** Updates a cada 30min foram apropriados (nÃ£o muito frequente, nÃ£o muito esparso)

---

## Metrics

### SLA Performance

| Metric                             | Target   | Actual | Met?   |
| ---------------------------------- | -------- | ------ | ------ |
| MTTD (Mean Time To Detection)      | < 5 min  | 2 min  | âœ… Yes |
| MTTR (Mean Time To Resolution)     | < 1 hour | 45 min | âœ… Yes |
| Communication (first notification) | < 30 min | 20 min | âœ… Yes |

### Incident Severity Justification

**Classified as:** P0 (Critical)

**Justification:**

- Sistema 100% indisponÃ­vel
- Todas as funcionalidades principais afetadas
- 100% dos usuÃ¡rios ativos impactados
- Downtime > 15 minutos

**Severity was appropriate:** âœ… Yes / âŒ No (explain if no)

---

## Supporting Information

### Relevant Links

- **Incident Slack Thread:** [Link]
- **GitHub Issue (Root Cause):** [Link]
- **Hotfix PR:** [Link]
- **Monitoring Dashboard:** [Link]
- **Railway Incident Log:** [Link]

### Related Incidents

- **Previous similar incidents:** [Listar se houver]
  - Exemplo: "2025-10-15 - Database timeout (P2) - diferente root cause mas sintomas similares"
- **Pattern identified:** [Se aplicÃ¡vel]

### References

- `docs/INCIDENT_RESPONSE.md` - Scenario 1: Database Down
- `DISASTER_RECOVERY.md` - Backup procedures
- `scripts/rollback.sh` - Rollback automation

---

## Sign-off

**Post-Mortem Author:** [Nome] - [Data]
**Reviewed by:** [Nome] - [Data]
**Approved by (Incident Commander):** [Nome] - [Data]

**Review Meeting:**

- **Date:** DD/MM/YYYY
- **Attendees:** [Lista de participantes]
- **Action Items Owner Assignment:** âœ… Complete

---

## Appendix

### Logs Excerpt

```
[2025-11-15 15:20:15] ERROR: Connection to database failed
[2025-11-15 15:20:16] ERROR: pg_connect(): could not connect to server
[2025-11-15 15:20:17] FATAL: out of memory
[2025-11-15 15:20:17] DETAIL: Failed on request of size 1024
```

### Query Analysis

```sql
-- Slow query sem Ã­ndice (identified as root cause):
EXPLAIN ANALYZE SELECT * FROM sections WHERE etp_id IN (1,2,3,...,50);

-- Execution time: 5.2s (antes do Ã­ndice)
-- Execution time: 0.05s (depois do Ã­ndice)
-- Speedup: 104x
```

### Timeline Diagram

```
15:20 â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€ Incident Start (DB crash)
            â”‚
15:22 â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€ Alert fired
            â”‚
15:25 â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€ Root cause identified
            â”‚
15:30 â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€ DB restarted (mitigation)
            â”‚
15:45 â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€ All-clear declared
            â”‚
16:05 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Users notified (resolution)

Total: 45 minutes (MTTR)
```

---

**Post-Mortem Version:** 1.0
**Template Version:** 1.0
**Last Updated:** 2025-11-15

---

## Usage Notes

**Mandatory sections:**

- Timeline
- Impact
- Root Cause
- Action Items

**Optional sections:**

- Supporting Information (helpful but not required)
- Appendix (use for deep technical details)

**Review timeline:**

- Draft: +24h apÃ³s resoluÃ§Ã£o
- Review meeting: +48h
- Final sign-off: +7 dias
- Action items completion: +30 dias
